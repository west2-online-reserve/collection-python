# **爬虫**



## *初识爬虫*

#### 爬虫合法性

###### 恶意爬虫：1.爬虫干扰访问网站的正常运营 	2.爬虫爬取了被保护的信息

###### 如何避免进局子：1.时常优化自己的程序，避免干扰被访问网站的正常运行	2.在爬取时候时常审核，触及敏感内容即使停止



#### 爬虫在使用场景中的分类

###### 通用爬虫：抓取系统重要组成部分，抓取一整张页面

###### 聚焦爬虫：建立在通用爬虫基础上，抓取特点内容

###### 增量式爬虫：检测网站数据更新情况，只抓取最新更新出来的数据



#### 反爬机制

###### 门户网站可以制定相关手段，防止爬虫程序爬取



#### 反反爬策略

###### 爬虫程序通过特定手段，破解反爬机制



#### Robots.txt协议

###### 君子协议：规定哪些可以爬取哪些不能 （网址/robot.txt查看）



#### http协议

###### 概念：服务器与客户端交互数据的一种形式

##### 常用请求头：

###### User-Agent: 请求载体的身份标识

###### Connection: 请求完毕后，是断开连接还是保持连接	

##### 常用响应头信息：

###### Content-Type: 服务器响应回客户端的数据类型



#### https协议

###### s--sercury 表示安全的超文本传输协议

###### 证书密钥加密



#### 加密方式

###### 对称密钥加密（客户端加密，密钥和密文一起发送给服务器）

###### 非对称密钥加密（服务器发送公钥（只能加密，不能解密），客户端将密文发送给服务器）【弊端：公钥可能被篡改，效率低】

###### 证书密钥加密（客户端将公钥给证书认证结构，审核后进行数字签名并封装到证书中再发送给服务器）





## *REQUESTS模块*

#### 什么是requests模块

###### python原生模块

###### 作用：模拟浏览器发送请求



#### 如何使用（编码流程）

###### 指定url

###### 发送https请求

###### 获取响应数据

###### 持久化存储



#### 环境安装

```python
pip install requests
```



#### 实战编码

```python
import requests
if __name__ == "__main__":
    #1
    url = 'https://www.baidu.com/'
    #2
    response = requests.get(url=url)
    #3
    page_text = response.text
    print(page_text)
    #4
    with open('./baidu.html','w',encoding='utf-8') as fp:
        fp.write(page_text)
    print("爬取结束")
```



```python
import requests
#反爬机制：UA检测：服务器门户会检查载体身份标识是否为浏览器
#反反爬策略：UA伪装
if __name__ == "__main__":
    #UA伪装：将对应User-Agent封装到一个字典中
    headers = {
         'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36'
    }
    url = 'https://www.baidu.com/s?wd'
    kw = input('enter a word:')
    params = {
        'query':kw
    }
    response = requests.get(url=url,params=params,headers=headers)
    page_text = response.text
    file_name = kw + '.html'
    with open(file_name,'w',encoding='utf-8') as fp:
        fp.write(page_text)
    print(file_name,"保存成功")
```

```python
import json

import requests
#post请求（携带了参数）
#响应数据是一组json数据
if __name__ == "__main__":
    headers = {
        'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0'
    }
    post_url = 'https://fanyi.baidu.com/sug'
    word = input()
    data = {
        'kw':word
    }
    response = requests.post(url=post_url,data=data,headers=headers)
    #获取响应数据，json数据返回是obj(要确认服务器返回数据)
    dic_ogj = response.json()
    file_name = word +'.json'
    fp = open(file_name,'w',encoding='utf-8')
    json.dump(dic_ogj,fp=fp,ensure_ascii=False)
    print("over")
```



```python

```

```python
import json

import requests

if __name__ == "__main__":
    url = 'https://movie.douban.com/j/chart/top_list'
    params = {'type': '24', 'interval_id': '100:90', 'action': '', 'start': '0',  # 从库中的第几个数开始取
        'limit': '20'  # 一次取的个数
    }
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36'}
    response = requests.get(url=url, params=params, headers=headers)
    data_list = response.json()
    fp = open('./douban.json', 'w', encoding='utf-8')
    json.dump(data_list, fp=fp, ensure_ascii=False)
    print("over")
```





## *数据解析*

#### 聚焦爬虫

###### 编码流程

###### - 指定url

###### - 发送请求

###### - 获取响应数据

###### - 数据解析

###### - 持久化储存



#### 数据解析分类

###### 正则表达式

###### bs4

###### xpath



#### 数据解析原理概述

###### 解析的局部文本内容都会再对应标签之间或者标签对应的属性中进行存储

###### 1.进行指定标签的定位

###### 2.标签或者标签对应的属性中存储的数据值进行提取（解析）



#### bs4进行数据解析

###### 实例化一个beautiful soup对象，并且将页面源码数据加载到该对象中

###### 通过调用Beautiful soup对象中相关属性或者方法进行标签定位和数据提取

###### 环境安装：

```python
pip install bs4
pip install lxml 
```

##### 对象实例化方式

###### 1.将本地hmtl文档加载到对象中

```python
fp = open('./test.html','r',encoding='utf-8')
BeautifulSoup(fp,'lxml')
```

###### 2.将互联网上获取的页面源码数据加载到对象中(更常用)

```python
page_text = response.text
soup = BeautifulSoup(page_text,'lxml')
```



##### 提供的用于数据解析的方法与属性

```python
soup.a
soup.div
#soup.tagName 返回的是html第一次出现的tagName对应的标签
soup.find('div') # ==soup.div
soup.find('div',class_='song') # 一定是class_,定位<div class='song'> 属性定位
soup.find_all('') # 可以找到符合要求的所有标签（返回列表）
soup.select('') # 参数为某种选择器（id，class，标签...），返回一个列表
				# 层选择器 >表示一个层级 空格表示多个层级
soup.a.text/string/get_text() # text/get_text()中所有的文本内容 string只可以获取该标签下直系的文本内容
soup.a['href'] # 直接获取标签属性值
```

##### 实战编码

```python
from bs4 import BeautifulSoup
import requests

if __name__ == "__main__":
    url = 'https://www.shicimingju.com/book/sanguoyanyi.html'
    headers = {
        'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36'
    }
    page_text = requests.get(url=url,headers=headers).text.encode('ISO-8859-1')
    soup = BeautifulSoup(page_text,'lxml')
    li_list = soup.select('.book-mulu > ul > li')
    fp = open('./sanguo.text','w',encoding='utf-8')
    for li in li_list:
        title = li.a.string
        detail_url = 'https://www.shicimingju.com' + li.a['href']
        detail_page_text = requests.get(url=detail_url,headers=headers).text.encode('ISO-8859-1')
        detail_soup = BeautifulSoup(detail_page_text,'lxml')
        div_tag = detail_soup.find('div',class_='chapter_content')
        content = div_tag.text
        fp.write(title+":"+content+"\n")
        print(title,"爬取成功")


```



## *XPATH解析*

###### 最常用且便捷高效的一种解析方式（通用性）

> xpath解析原理：
>
> 1.实例化一个etree对象，且需要将被解析的页面源码数据加载到对象中
>
> 2.调用etree对象中的xpath方法结合着xpath表达式实现标签的定位和内容的捕获



#### 如何实例化一个etree对象

> 1.将本地的html文档中的源码数据加载到etree对象中:
>
> etree.parse(filePath)
>
> 2.可以将从互联网上获取的源码数据加载到对象中
>
> etree.HTML('page_text')



#### xpath表达式

```python
```

